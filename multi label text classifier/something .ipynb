{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Label Text Classifier using word embeddings and LSTM\n",
    "    NLP uses deep learning to get great results. One sort of NLP is text classification.\n",
    "    Text classification is used to determine type of text that machine is handling. \n",
    "    \n",
    "    Keras is a framework to implement deep learning models in a easy and fast manner. \n",
    "    Tensor flow, Theano, CNTK can be used as keras's backend. \n",
    "    \n",
    "* Traditionally in NLP, words are encoded to ID and given to data. But these dont preserve contextual information. So, word embeddings are used. Every word will have a vector and its present in vector space of whole corpus. Simply, imagine word 'The' is represented as a vector and not as ID.\n",
    "* LSTM RNN solve exploding and vanishing gradients. \n",
    "\n",
    "I use the above starred points to solve \"multi label text classifier\"\n",
    "\n",
    "Steps:\n",
    "1. Preprocess the data and transformation of data. \n",
    "2. Create word embedding model.\n",
    "3. Create a simpler LSTM model (many to one) and train the data\n",
    "4. Check the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Preprocessing the data\n",
    "* Before preprocessing the data, the engineer has to look at the data in its raw format and decide how to preprocess it. \n",
    "* Me, after looking at the data. \n",
    "* I decided to remove the word \"BOS\", \"EOS\", numbers & special characters \" ' \",\" . \" (Best Practise is to  remove numericals and special characters )\n",
    "* Each line contains request and its label, they are seperated by a tab space. \n",
    "* I split the data into request and label\n",
    "* I removed new line characters from labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "request = []\n",
    "labels = []\n",
    "\n",
    "f = open('final_raw.txt','r')\n",
    "for line in iter(f):\n",
    "    line = line.replace(\"BOS \", \"\")\n",
    "    line = line.replace(\" EOS\", \"\")\n",
    "    line = re.sub('[0-9]+', '', line)\n",
    "    line = line.replace(\"'\", \"\")\n",
    "    line = line.replace(\".\", \"\")\n",
    "    line = line.split('\\t')\n",
    "    temp = line[0].split(' ')\n",
    "    request.append(temp)\n",
    "    line[1] = line[1].replace(\"\\n\",\"\")\n",
    "    labels.append(line[1])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lets have some look at the data \n",
    "* request\n",
    "* labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['i',\n",
       "  'want',\n",
       "  'to',\n",
       "  'fly',\n",
       "  'from',\n",
       "  'baltimore',\n",
       "  'to',\n",
       "  'dallas',\n",
       "  'round',\n",
       "  'trip'],\n",
       " 'atis_flight')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "( request[0], labels[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data Transformation\n",
    "Lets Encode the labels. so that it can be given to model as integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_labels = LabelEncoder()\n",
    "labels_encoded = labelencoder_labels.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create word embeddings.\n",
    "    \n",
    "You can word embeddings in 2 ways:\n",
    "\n",
    "1. Create your own word embedding model  given text dataset.\n",
    "2. Use Google's 'word2vec' embedding model or use 'glove' word embedding model \n",
    "\n",
    "When using 'word2vec' and 'glove' embedding model, you need to remove words from text dataset, which are not present in these embedding models. \n",
    "\n",
    "I prefer to train my model on text corpus, believing that it captures more context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gensim is a python library that creates word embeddings for a given text corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chaitra\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = np.asarray(request)\n",
    "# train model\n",
    "model_vec = Word2Vec(request, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=712, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# summarize the loaded model\n",
    "print(model_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'charlotte', 'plane', 'montreal', 'lunch', 'kansas', 'that', 'reaches', 'jose', 'philadelphia', 'mornings', 'rental', 'st', 'florida', 'connection', 'they', 'friday', 'whatre', 'advertises', 'hi', 'october', 'thursday', 'francisco', 'may', 'airports', 'anywhere', 'these', 'airport', 'amount', 'toward', 'least', 'sundays', 'near', 'approximately', 'new', 'airplane', 'ill', 'hello', 'eastern', 'boeing', 'cars', 'january', 'wednesday', 'express', 'sixteenth', 'explain', 'trans', 's', 'gets', 'the', 'looking', 'fn', 'rate', 'eye', 'service', 'capacities', 'arrival', 'describe', 'april', 'jfk', 'flight', 'august', 'people', 'meals', 'direct', 'supper', 'los', 'houston', 'twenty', 'eighth', 'planes', 'their', 'reservation', 'petersburg', 'snack', 'offer', 'repeat', 'know', 'york', 'week', 'thats', 'cincinnati', 'most', 'spend', 'no', 'lets', 'charges', 'next', 'world', 'put', 'another', 'departure', 'day', 'missouri', 'coach', 'airlines', 'tuesdays', 'shortest', 'departing', 'co', 'somebody', 'dinnertime', 'greatest', 'cp', 'being', 'bur', 'milwaukee', 'total', 'y', 'difference', 'salt', 'atl', 'ohio', 'along', 'limousines', 'ones', 'within', 'delta', 'fifteenth', 'it', 'more', 'red', 'provide', 'eleventh', 'train', 'operation', 'dinner', 'pittsburgh', 'fort', 'via', 'thirty', 'north', 'itinerary', 'j', 'also', 'names', 'time', 'type', 'stands', 'when', 'stops', 'bna', 'number', 'flying', 'up', 'airline', 'without', 'friends', 'maximum', 'find', 'bwi', 'stopping', 'intercontinental', 'having', 'arrangements', 'international', 'live', 'take', 'were', 'seating', 'companies', 'distance', 'offers', 'f', 'represented', 'july', 'should', 'doesnt', 'enroute', 'eighteenth', 'denver', 'inexpensive', 'seventeen', 'any', 'originating', 'depart', 'orlando', 'yorks', 'returning', 'november', 'ff', 'make', 'oh', 'layover', 'hou', 'while', 'expensive', 'how', 'stop', 'united', 'transcontinental', 'burbank', 'local', 'for', 'prefer', 'ticket', 'your', 'but', 'various', 'tell', 'uses', 'earliest', 'eleven', 'last', 'turboprop', 'weekdays', 'served', 'includes', 'philly', 'proper', 'kind', 'arrive', 'arrivals', 'northwest', 'american', 'economic', 'will', 'everywhere', 'us', 'continent', 'ua', 'to', 'about', 'hopefully', 'abbreviation', 'and', 'too', 'or', 'listed', 'single', 'so', 'used', 'canadian', 'trying', 'yes', 'begins', 'june', 'final', 'eight', 'ontario', 'look', 'mitchell', 'okay', 'san', 'travels', 'connections', 'belong', 'ds', 'closest', 'jet', 'q', 'afternoons', 'la', 'columbus', 'actually', 'lester', 'possible', 'leaving', 'dollars', 'vicinity', 'dallas', 'here', 'let', 'monday', 'run', 'qualify', 'schedule', 'than', 'today', 'cheapest', 'm', 'designate', 'third', 'makes', 'stand', 'listing', 'you', 'abbreviations', 'through', 'seventeenth', 'quebec', 'limousine', 'plan', 'wanted', 'wish', 'be', 'got', 'year', 'one', 'costs', 'regarding', 'indiana', 'lives', 'nw', 'i', 'far', 'leaves', 'fourteenth', 'from', 'earlier', 'b', 'still', 'during', 'listings', 'dl', 'saturday', 'just', 'symbols', 'boston', 'sd', 'tuesday', 'need', 'am', 'id', 'march', 'lands', 'schedules', 'ea', 'morning', 'into', 'transport', 'what', 'services', 'nighttime', 'flights', 'airplanes', 'are', 'either', 'between', 'mia', 'connecting', 'name', 'louis', 'paul', 'ac', 'tampa', 'minneapolis', 'las', 'calling', 'a', 'nashville', 'would', 'ap', 'over', 'less', 'working', 'ord', 'preferably', 'serves', 'can', 'in', 'ground', 'takeoffs', 'phoenix', 'coming', 'scenario', 'first', 'late', 'qx', 'breakfast', 'fifth', 'mealtime', 'tacoma', 'list', 'each', 'including', 'book', 'dfw', 'rent', 'sixteen', 'place', 'ls', 'serviced', 'detroit', 'flies', 'ninth', 'laying', 'carried', 'restrictions', 'downtown', 'oakland', 'out', 'area', 'inform', 'december', 'buy', 'toronto', 'three', 'angeles', 'usa', 'trips', 'define', 'wants', 'goes', 'reservations', 'starting', 'michigan', 'scheduled', 'information', 'lufthansa', 'america', 'sunday', 'close', 'county', 'h', 'pennsylvania', 'some', 'go', 'seven', 'logan', 'around', 'sure', 'baltimore', 'give', 'instead', 'level', 'city', 'help', 'after', 'numbers', 'bring', 'arrange', 'where', 'connect', 'order', 'great', 'miami', 'georgia', 'meaning', 'an', 'pearson', 'by', 'long', 'under', 'cost', 'days', 'seats', 'interested', 'ive', 'hours', 'landing', 'aircraft', 'using', 'catch', 'twa', 'evening', 'midway', 'qo', 'texas', 'originate', 'nationair', 'lake', 'priced', 'see', 'then', 'arriving', 'cities', 'guardia', 'restriction', 'cleveland', 'its', 'wednesdays', 'thereafter', 'memphis', 'now', 'booking', 'going', 'rates', 'following', 'noontime', 'im', 'passengers', 'alaska', 'stopover', 'six', 'fine', 'mondays', 'such', 'pm', 'later', 'aa', 'departs', 'on', 'is', 'rentals', 'come', 'again', 'newark', 'midwest', 'those', 'at', 'very', 'before', 'available', 'field', 'thirteenth', 'indianapolis', 'have', 'beach', 'ten', 'this', 'meal', 'staying', 'worth', 'sometime', 'midnight', 'general', 'non', 'of', 'fares', 'besides', 'california', 'equal', 'both', 'say', 'ewr', 'bound', 'early', 'destination', 'business', 'carries', 'options', 'minnesota', 'airfare', 'departures', 'types', 'traveling', 'want', 'currently', 'lastest', 'fourth', 'who', 'round', 'four', 'sfo', 'provides', 'latest', 'southwest', 'route', 'has', 'soon', 'show', 'whats', 'smallest', 'thank', 'oak', 'classes', 'stapleton', 'lowest', 'love', 'sa', 'once', 'leave', 'fridays', 'seattle', 'there', 'fare', 'different', 'thursdays', 'other', 'located', 'include', 'all', 'please', 'reverse', 'canada', 'takes', 'yn', 'date', 'cheap', 'capacity', 'zone', 'arizona', 'across', 'dont', 'tenth', 'stopovers', 'my', 'utah', 'code', 'could', 'runs', 'washington', 'two', 'start', 'only', 'able', 'serving', 'display', 'dulles', 'else', 'c', 'thrift', 'visit', 'atlanta', 'largest', 'oclock', 'westchester', 'nonstops', 'limo', 'town', 'me', 'thing', 'jersey', 'with', 'taxi', 'trip', 'reaching', 'locate', 'economy', 'use', 'deltas', 'concerning', 'as', 'seventh', 'qw', 'choices', 'daily', 'twentieth', 'many', 'noon', 'nineteenth', 'afternoon', 'tomorrow', 'landings', 'requesting', 'americans', 'd', 'takeoff', 'like', 'atlantas', 'get', 'must', 'weekday', 'february', 'september', 'sorry', 'hartfield', 'continuing', 'travel', 'class', 'operating', 'month', 'equipment', 'beginning', 'nonstop', 'database', 'right', 'way', 'codes', 'arrives', 'hp', 'car', 'nevada', 'which', 'tennessee', 'diego', 'tower', 'repeating', 'vegas', 'home', 'serve', 'twelfth', 'price', 'taking', 'times', 'carolina', 'offered', 'heading', 'second', 'fit', 'tickets', 'well', 'colorado', 'dc', 'if', 'directly', 'prices', 'does', 'highest', 'question', 'return', 'much', 'sixth', 'mean', 'them', 'chicago', 'kinds', 'night', 'mco', 'thirtieth', 'airfares', 'saturdays', 'land', 'grounds', 'provided', 'transportation', 'west', 'back', 'continental', 'making', 'do', 'sort', 'fly', 'whether', 'iah', 'minimum', 'yyz', 'same', 'air', 'straight', 'connects']\n",
      "712\n"
     ]
    }
   ],
   "source": [
    "# summarize vocabulary\n",
    "words = list(model_vec.wv.vocab)\n",
    "print(words)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looking at the vector of the word 'trip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.65366089 -0.03637737 -0.05059821 -0.0171887   0.0455776   0.38865346\n",
      " -0.16616102  0.04226137 -0.06991297  0.0603858   0.12815918  0.01084718\n",
      "  0.09571978  0.36944807 -0.25016913 -0.24622206  0.00678377  0.41599265\n",
      " -0.23819438 -0.06295195  0.14006706  0.08455258  0.00498152  0.15439513\n",
      "  0.21382695 -0.00066623 -0.07322553 -0.17985065 -0.41274625  0.00866815\n",
      "  0.24029277 -0.35575405  0.49114737 -0.00744834 -0.01922256 -0.07308241\n",
      "  0.41536006  0.21369734 -0.46222466  0.23555312  0.06116557 -0.08631554\n",
      "  0.09496447  0.26096189  0.08722508 -0.1143515  -0.25571516 -0.44265786\n",
      " -0.09551213  0.44629547  0.1956607  -0.32173955  0.0765483  -0.48595366\n",
      " -0.11774267 -0.38030055  0.28741893 -0.30913919 -0.09047089 -0.1554341\n",
      "  0.62935293  0.13833532  0.29244468  0.07281834 -0.17708285  0.25091544\n",
      " -0.06994165  0.47397357 -0.34605703  0.13467559 -0.0412305  -0.12756269\n",
      " -0.24033183  0.09237041 -0.27368605 -0.19603567 -0.24889509  0.20308998\n",
      " -0.0433677   0.0097833  -0.40524617 -0.54449654 -0.18166609 -0.180407\n",
      " -0.02118921  0.11433068 -0.21832778 -0.40206486 -0.10905118 -0.02579564\n",
      " -0.37394211 -0.02104924 -0.02124142  0.02253422  0.00212155  0.10350541\n",
      " -0.1671911   0.17406553 -0.08290081 -0.47228941]\n"
     ]
    }
   ],
   "source": [
    "# access vector for one word\n",
    "print(model_vec['trip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the statistics of the corpus we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46, 1, 11.280946065428823)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req_lens = []\n",
    "for i in range(0,len(request)):\n",
    "    req_lens.append( len(request[i]))\n",
    "    \n",
    "aa = np.asarray(req_lens)\n",
    "(aa.max(), aa.min(),aa.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistics here and in the previous notebook doesnt match. IDK why. Lets move on. I will have a look at the statistics later, as it is of less priority."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got to know that average no of words is 11. \n",
    "\n",
    "I will have twice the average as row matrix. That means the data is trimmed to 20 words per each sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(request)):\n",
    "    request[i] = request[i][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not going to distrub the original corpus . I create a zero numpy array of shape (4524,20,100) i.e 4524 sentences, 20 words, 100 vectors of each words\n",
    "\n",
    "This eliminates the step of padding each sentence. Quite wonderful, isnt it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hh  = np.zeros(shape=(4524,20,100))\n",
    "for i in range(len(request)):\n",
    "    for j in range(len(request[i])):\n",
    "        hh[i][j] = model_vec[request[i][j] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the shape, word and its vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4524, 20, 100),\n",
       " 'want',\n",
       " array([-0.7389788 , -0.04055923,  0.08641576,  0.12363553,  0.05098152,\n",
       "         0.17420503, -0.25286803,  0.02319203, -0.08540826, -0.03309162,\n",
       "         0.11532205, -0.03572461, -0.01597139,  0.32943872, -0.06826755,\n",
       "        -0.11031719, -0.02668942,  0.30229938, -0.08880879,  0.01215467,\n",
       "         0.05174172,  0.1629716 , -0.14065567,  0.2304832 ,  0.03587618,\n",
       "         0.06667246,  0.05996433, -0.29377291, -0.38158241, -0.09248393,\n",
       "         0.05427686, -0.23276503,  0.4110254 , -0.08192627, -0.13393882,\n",
       "        -0.28103629,  0.27582783,  0.28076112, -0.30359653,  0.05260054,\n",
       "        -0.01397794, -0.00971313,  0.1106336 ,  0.21021052,  0.2282887 ,\n",
       "        -0.09043355, -0.31616017, -0.40376154, -0.05114204,  0.18393269,\n",
       "         0.28197241, -0.22838278, -0.0699352 , -0.34867066, -0.07239054,\n",
       "        -0.26409933,  0.03571077, -0.26893169,  0.02000365, -0.18976863,\n",
       "         0.35511482, -0.04331189,  0.22344229,  0.04168072, -0.09777664,\n",
       "         0.10513742, -0.04892923,  0.35405609, -0.42819428,  0.23338294,\n",
       "        -0.0731364 , -0.07925549, -0.28938395,  0.00832494, -0.10593261,\n",
       "        -0.13127171, -0.28457984,  0.33160758,  0.02606293,  0.25382558,\n",
       "        -0.44266722, -0.49365041, -0.04442395,  0.00086865, -0.06560582,\n",
       "         0.20200199, -0.23616825, -0.22309853,  0.0371703 , -0.05177584,\n",
       "        -0.27013984, -0.0458926 , -0.0855468 ,  0.21272026, -0.06146971,\n",
       "         0.0311357 , -0.07231869,  0.18774591, -0.05634045, -0.28184953]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(hh.shape, request[0][1], hh[0][1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Creating LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm = Sequential([\n",
    "    LSTM(20,input_shape=(20,100) ),\n",
    "    Dense(100, activation='softmax'),\n",
    "    Dense(20, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam',    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3619 samples, validate on 905 samples\n",
      "Epoch 1/3\n",
      "3619/3619 [==============================] - 7s - loss: nan - acc: 0.0481 - val_loss: 2.9957 - val_acc: 0.0298\n",
      "Epoch 2/3\n",
      "3619/3619 [==============================] - 5s - loss: nan - acc: 0.0287 - val_loss: 2.9957 - val_acc: 0.0298\n",
      "Epoch 3/3\n",
      "3619/3619 [==============================] - 5s - loss: nan - acc: 0.0287 - val_loss: 2.9957 - val_acc: 0.0298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e148300cf8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.fit(hh, labels_encoded, \n",
    "          batch_size=32, \n",
    "          epochs=3,\n",
    "          validation_split = 0.2\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy, val_loss, val_acc are not improving for each epoch.\n",
    "\n",
    "Some ways to solve\n",
    "1. Use suitable optimizer.\n",
    "2. Use suitable loss function. \n",
    "3. Use regularizers, dropouts\n",
    "4. Try different learning rates. \n",
    "\n",
    "Will try all these soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another Interesting thing is acc, val_loss, val_acc are same for both 'lstm with word embedding model' * 'lstm without word embedding model ' though the architectures are completely different. How, fascinationg. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting. \n",
    "\n",
    "Here is the sentence that i want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sentence = 'ground transportation in salt like city'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the sentence :\n",
    "1. split the sentence into tokens (words)\n",
    "2. create a numpy zero test vector \n",
    "3. go through each word, get the vector of each word, and store it in the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#splitting using space as seperator\n",
    "test_sentence = test_sentence.split(' ')\n",
    "#creating a numpy zero matrix\n",
    "test_vec = np.zeros(shape=(1,20,100))  \n",
    "# getting vector of the sentence\n",
    "for kk in range(len(test_sentence)):\n",
    "    test_vec[0][kk] = model_vec[test_sentence[kk]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = lstm.predict(test_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "         nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "( preds )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorry for the useless model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additonal \n",
    "1. You can save the embedding model and re use it in other projects by loading it back. \n",
    "   * If you wanted to use the other popular embedding models you can load it. (like 'word2vec ' and 'glove'\n",
    "2. Its wise to dump the data at multiple stages using 'pickle'. You can load the preprocessed data using pickle and try out differnt models on it. I have not used here, as its a small project. \n",
    "3. YOu can use bcolz to save large arrays. Bcolz saves the array to local disk and this solves a whole lot of time . \n",
    "3. You can save the LSTM model and load the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save embeddign model in binary format\n",
    "model_vec.save('model_vec.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading embeddingmodel \n",
    "# apparently this gives error now and i dont know why. \n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format('model_vec.bin', binary=True)  # C binary format"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
