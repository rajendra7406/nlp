{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Label Text Classifier using LSTM\n",
    "    NLP uses deep learning to get great results. One sort of NLP is text classification.\n",
    "    Text classification is used to determine type of text that machine is handling. \n",
    "    \n",
    "    Keras is a framework to implement deep learning models in a easy and fast manner. \n",
    "    Tensor flow, Theano, CNTK can be used as keras's backend. \n",
    "    \n",
    "* Traditionally in NLP, words are encoded to ID and given to data. \n",
    "* LSTM RNN solve exploding and vanishing gradients. \n",
    "\n",
    "I use the above starred points to solve \"multi label text classifier\"\n",
    "\n",
    "Steps:\n",
    "1. Preprocess the data\n",
    "2. Prepare data to desired format. So, that it can be sent to model\n",
    "3. Create a simpler LSTM model (many to one) and train the data\n",
    "4. Check the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Preprocessing the data\n",
    "* Before preprocessing the data, the engineer has to look at the data in its raw format and decide how to preprocess it. \n",
    "* Me, after looking at the data. \n",
    "* I decided to remove the word \"BOS\", \"EOS\", numbers & special characters \" ' \",\" . \" (Best Practise is to  remove numericals and special characters )\n",
    "* Each line contains request and its label, they are seperated by a tab space. \n",
    "* I split the data into request and label\n",
    "* I removed new line characters from labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "request = []\n",
    "labels = []\n",
    "import re\n",
    "f = open('final_raw.txt','r')\n",
    "for line in iter(f):\n",
    "    line = line.replace(\"BOS \", \"\")\n",
    "    line = line.replace(\" EOS\", \"\")\n",
    "    line = re.sub('[0-9]+', '', line)\n",
    "    line = line.replace(\"'\", \"\")\n",
    "    line = line.replace(\".\", \"\")\n",
    "    line = line.split('\\t')\n",
    "    request.append(line[0])\n",
    "    line[1] = line[1].replace(\"\\n\",\"\")\n",
    "    labels.append(line[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have some look at the data \n",
    "* request\n",
    "* labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('i want to fly from baltimore to dallas round trip', 'atis_flight')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "( request[0], labels[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Encode the labels. so that it can be given to model as integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_labels = LabelEncoder()\n",
    "labels = labelencoder_labels.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4524,), (), (), ())"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(labels.shape, labels[0].shape, labels[89].shape, labels[489].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that labels have just 4524 rows and 1 column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lets see total no of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transforming requests**\n",
    "* Tokenising each data. Is simple words, each word is converted to integer. \n",
    "* If word \"the\" is the highest repeated word in the whole dataset. It will be tokenised as 1. \n",
    "  If word \"if\" is the second highest repeated word in the whole dataset. It will be tokenised as 2. \n",
    "* The code is written below. ( 'num_words' is the num of the unique words it will deal. rest are thrown away )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.preprocessing.text\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "tk = keras.preprocessing.text.Tokenizer(num_words=2000,\n",
    "                                   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                   lower=True,\n",
    "                                   split=\" \",\n",
    "                                   char_level=False)\n",
    "tk.fit_on_texts(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "req_vec = tk.texts_to_sequences(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at tokenised requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 6, 4, 62, 2, 20, 1, 11, 14]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req_vec[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4524,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "req_vec = np.asarray(req_vec)\n",
    "( req_vec.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "having a detailed look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 14, 9, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "( len(req_vec[0]), len(req_vec[50]), len(req_vec[100]), len(req_vec[1000]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\" request \" contains all the queries \n",
    "Lets do some simple statistics on \" request\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(255, 3, 63.291777188328915)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "req_lens = []\n",
    "for i in range(0,len(request)):\n",
    "    req_lens.append( len(request[i]))\n",
    "    \n",
    "aa = np.asarray(req_lens)\n",
    "(aa.max(), aa.min(),aa.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are:\n",
    "* 255 words at max\n",
    "* 3 words at min\n",
    "* 63 words at average \n",
    "\n",
    "The significance of this result will be explained later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Based on the average no of words in each sentence, I decide input matrix. I usually do the twice of average words and keep that as the column size\n",
    "* Then I pad sentence with less words with zeros. So, I can have a matrix of a finite shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 120\n",
    "req_vec = sequence.pad_sequences(req_vec,maxlen=max_len, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 120, 120, 120)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "( len(req_vec[0]), len(req_vec[50]), len(req_vec[100]), len(req_vec[1000]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data after transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4524, 120),\n",
       " 'show me the fares from dallas to san francisco',\n",
       " array([10,  6,  4, 62,  2, 20,  1, 11, 14,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "( req_vec.shape, request[100], req_vec[100] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Builiding the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense,Dense, Activation, LSTM\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chaitra\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 800\n",
    "cols = 120\n",
    "batch  =1\n",
    "model = Sequential([\n",
    "            Embedding(vocab_size, batch, input_length=cols, dropout=0.2),\n",
    "            LSTM(500),\n",
    "            Dense(256),\n",
    "            Dense(128),\n",
    "            Dense(20,activation='softmax')\n",
    "                  ])\n",
    "#labels are encoded and that is the reason i use sparse categorical cross entropy. Nothing special\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model with 3 epochs to have a quick look.\n",
    "\n",
    "With keras you can split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3619 samples, validate on 905 samples\n",
      "Epoch 1/3\n",
      "3619/3619 [==============================] - 88s - loss: nan - acc: 0.0594 - val_loss: 2.9957 - val_acc: 0.0298\n",
      "Epoch 2/3\n",
      "3619/3619 [==============================] - 86s - loss: nan - acc: 0.0287 - val_loss: 2.9957 - val_acc: 0.0298\n",
      "Epoch 3/3\n",
      "3619/3619 [==============================] - 86s - loss: nan - acc: 0.0287 - val_loss: 2.9957 - val_acc: 0.0298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x265abef3e48>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(req_vec, labels, \n",
    "          batch_size=32, \n",
    "          epochs=3,\n",
    "          validation_split = 0.2\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -\n",
    "The reason is accuracy, val_loss, val_acc are not improving for each epoch.\n",
    "\n",
    "Some ways to solve\n",
    "1. Use suitable optimizer.\n",
    "2. Use suitable loss function. \n",
    "3. Use regularizers, dropouts\n",
    "4. Try different learning rates. \n",
    "\n",
    "Will try all these soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets predict\n",
    "* Copy one of the tokenised request and lets have a quick prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 120)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 120)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing = [[10,  6,  4, 62,  2, 20,  1, 11, 14,  0,  0,  0,  0,  0,  0,  0,  0,\n",
    "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
    "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
    "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
    "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
    "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
    "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
    "         0] ]\n",
    "testing = np.asarray(testing)\n",
    "print(testing.shape)\n",
    "testing.reshape(1,120)\n",
    "print(type(testing) )\n",
    "testing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "         nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = ' any flights from dallas to salt city'\n",
    "# Tokenizing\n",
    "sen_vec = tk.texts_to_sequences(sentence)\n",
    "# Padding\n",
    "sen_vec = sequence.pad_sequences(req_vec,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "         nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Sorry for the useless model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
